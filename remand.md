图像风格转换：V1,V2, V3
VGG_content.py 是对VGG16模型内容的查看，CNN抽取更抽象的语义特征
style_transfer.py 是图像风格转换，输出 = 内容 + 风格

1.使用VGG16预处理模型，得到每层卷积的输出（特征提取），之后进行风格和内容的加权生成图片
2.风格中使用了Gram矩阵，具体自己查
3.V2改进V1，是在输入做了改变，V2还可以做图像超清化处理
    a).V1的方式，需要每次都数据初化那张图片，之后用梯度下降的算法求解
        为了得到比较的好结果，需要多次运行梯度下降算法，这样效率就会变低
    b).V2进行改进，就是初化化的图片不在随机，而是经过一个网络（针对某个风格建立的网络）得到
        V1,V2的损失函数计算方式都是一样，通过预训练好的VGG16。
4.V2网络细节
    a).不使用pooling层，使用strided和fractionally strided卷积
        因为卷积比起池化能更多保存数据信息
    b).使用5个residual blocks，
        因为残差块更能保持输入图片的数据信息
    c).输出层使用的是scaled tanh保证输出值是在[0,255]
        在syle_transfer.py中输出值是裁剪了，可以通过
        控制激活函数来控制输出值的范围
    d).第一个和最后一个卷积核都使用的是9*9，其他使用3*3
    e).为啥先down_samping再做up_samping?
        减少了feature_map的大小，提高性能
        提高结果图像中的视野域，风格转换会导致物体形变，因而
            图像对应的初始结果中的视野越大越好
    f).为什么使用residual connenction(残差连接)
        输入、输出可以共享一部分信息
        帮助学到需要变换的部分
5.V1, V2的缺点
    都引入了风格损失，它是很难定义的（没有数学上的定义证明
    于是引入V3，解决Gram矩阵虽然有效，但是直观，无从解释
6.V3--Match小块分割法
    将图像切除小块，相互之间可以重叠，最后在拼接成一个图片的时候
    有交叠的地方做一个均值。（即块可以重叠也可独立）
    a).内容损失，都是对应的小块之间求
    b).风格损失，是找最接近的(用余弦方法)，(K-means聚类？)
        而非内容损失那样找对应的块，找到后
        它不需要像V1, V2那样求Gram，是用内容损失那种计算公式
7.三中风格转换算法对比
    a).V1, 效率低，但是效果会好
    b).V2, 效率高，使用，效果略低
    c).V3, 解决了Gram矩阵的问题,重新定义了风格损失函数，效果更好